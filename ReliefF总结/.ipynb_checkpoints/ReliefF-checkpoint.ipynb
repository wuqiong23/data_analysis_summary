{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relief系列算法总结\n",
    "&emsp;&emsp;在机器学习的实际应用中，特征数量可能较多，其中可能存在不相关的特征，特征之间也可能存在相关性，容易导致如下的后果：\n",
    "1. 特征个数越多，分析特征、训练模型所需的时间就越长，模型也会越复杂。\n",
    "2. 特征个数越多，容易引起“维度灾难”，其推广能力会下降。\n",
    "3. 特征个数越多，容易导致机器学习中经常出现的特征稀疏的问题，导致模型效果下降。\n",
    "4. 对于模型来说，可能会导致不适定的情况，即是解出的参数会因为样本的微小变化而出现大的波动。<br>\n",
    "\n",
    "特征选择，能剔除不相关、冗余、没有差异刻画能力的特征，从而达到减少特征个数、减少训练或者运行时间、提高模型精确度的作用。\n",
    "Relief是一系列的特征权重算法，包括Relief（二分类），ReliefF（多分类）以及RReliefF（目标属性为连续值的回归问题）。本文仅介绍前两者。\n",
    "\n",
    "\n",
    "## Relief思想\n",
    "&emsp;&emsp;根据各个特征和类别的相关性赋予特征不同的权重，权重小于某个阈值的特征将被移除。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理\n",
    "&emsp;&emsp;Relief算法中特征和类别的相关性是基于特征对近距离样本的区分能力。\n",
    "### Relief算法\n",
    "&emsp;&emsp;算法从训练集D中随机选择一个样本R，然后从和R同类的样本中寻找最近邻样本H，称为Near Hit，从和R不同类的样本中寻找最近邻样本M，称为NearMiss，然后根据以下规则更新每个特征的权重：如果R和Near Hit在某个特征上的距离小于R和Near Miss上的距离，则说明该特征对区分同类和不同类的最近邻是有益的，则增加该特征的权重；反之，如果R和Near Hit在某个特征的距离大于R和Near Miss上的距离，说明该特征对区分同类和不同类的最近邻起负面作用，则降低该特征的权重。以上过程重复m次，最后得到各特征的平均权重。特征的权重越大，表示该特征的分类能力越强，反之，表示该特征分类能力越弱。<br>\n",
    "\n",
    "&emsp;&emsp;假设一个样例X有p个特征，S为样本量为n的训练样本集，F即${f_{1},f_{2},...f_{p}}$为特征集，一个样例X由p维向量$(x_{1},x_{2},...x_{p})$构成，其中，$x_{j}$为X的第j个特征的值。\n",
    "&emsp;&emsp;relief算法可以解决名义变量和数值变量，两个样例X和Y的特征的值的差可由下面的函数定义：\n",
    "\n",
    "&emsp;&emsp;当$x_{k}$和$y_{k}$为名义变量时：\n",
    "$$diff(x_{k},y_{k})= \\begin{cases}\n",
    "0,如果x_{k}和y_{k}相同\\\\\n",
    "1,如果x_{k}和y_{k}不同\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "&emsp;&emsp;当$x_{k}$和$y_{k}$为数值变量时：\n",
    "$$diff(x_{k},y_{k})= (x_{k}-y_{k})/v_{k}$$\n",
    "$v_{k}$为归一化单位，把diff值归一到[0,1]区间，可以在之前先把数值变量进行归一化。\n",
    "\n",
    "&emsp;&emsp;<font color = red size = 4>算法流程如下：</font><br>\n",
    "\n",
    "输入：样本集S，抽样次数m，特征权重阈值$\\tau$<br>\n",
    "输出：选择后的特征集\n",
    "+ 把S分成$S^{+}={正例}$和$S^{-}={负例}$\n",
    "+ 权重W=(0,0,...0)\n",
    "+ For i = 1 to m\n",
    "    * 随机选择一个样例$X \\in S$\n",
    "    * 随机选择一个距离X最近邻的正例$Z^{+} \\in S^{+}$\n",
    "    * 随机选择一个距离X最近邻的负例$Z^{-} \\in S^{-}$\n",
    "    * if X是一个正例\n",
    "        - then Near_hit = $Z^{+}$; Near_miss = $Z^{-}$\n",
    "    * else Near_hit = $Z^{-}$; Near_miss = $Z^{+}$\n",
    "    * for i = 1 to p\n",
    "        - $W_{i} = W_{i} - diff(x_{i},near\\_hit_{i})^{2} + diff(x_{i},near\\_miss_{i})^{2}$\n",
    "\n",
    "+ relevance = $\\frac{W}{m}$\n",
    "+ for i = 1 to p\n",
    "    * if $relevance_{i} \\geq \\tau$\n",
    "        - then $f_{i}$是一个相关的特征\n",
    "    * else $f_{i}$是一个不相关的特征 \n",
    "        \n",
    "&emsp;&emsp;relief在下列情况有效：（1）相关性水平对于相关的特征很大，对于不相关的特征很小，（2）$\\tau$用来选择相关特征，去除不相关特征。<br> \n",
    "&emsp;&emsp;计算复杂度：$\\Theta(pmn)$，p为特征数，m为迭代次数，n为样例数。可以看出该算法的运行时间随着样本的抽样次数m和原始特征个数p的增加线性增加，因此运行效率非常高。<br>\n",
    "\n",
    "### ReliefF算法\n",
    "&emsp;&emsp;ReliefF算法在处理多类问题时，每次从训练样本集中随机取出一个样本R，然后从和R同类的样本集中找出R的k个近邻样本(near Hits)，从每个R的不同类的样本集中均找出k个近邻样本(near Misses)，然后更新每个特征的权重，如下式所示：\n",
    "$$W(A)=W(A)-\\sum_{j=1}^{k}\\frac{A,R,H_{j}}{mk}+\\sum_{C \\notin class(R)}[\\frac{p(C)}{1-p(class(R))} \\sum_{j=1}^{k}diff(A,R,M_{j}(C))]/(mk)$$\n",
    "\n",
    "&emsp;&emsp;上式中，$diff(A,R_{1},R_{2})$表示样本$R_{1}$和$R_{2}$在特征A上的差，$M_{j}(C)$表示类$C \\notin class(R)$中第j个最近邻样本。如下式：\n",
    "$$diff(A,R_{1},R_{2}) = \\begin{cases}\n",
    "\\frac{|R_{1}[A]-R_{2}[A]|}{max(A)-min(A)},如果A是连续的\\\\\n",
    "\\begin{cases}\n",
    "0,如果A是离散的，且 R_{1}[A]=R_{2}[A]\\\\\n",
    "1,如果A是离散的，且 R_{1}[A]\\neq R_{2}[A]\\\\\n",
    "\\end{cases}\n",
    "\\end{cases}$$\n",
    "\n",
    "&emsp;&emsp;<font color = red size = 4>算法流程如下：</font><br>\n",
    "输入：训练集D，抽样次数m，特征权重阈值$\\tau$，最近邻样本个数k\n",
    "输出：各个特征的特征权重T\n",
    "+ 置所有特征权重为0，T为空集\n",
    "+ for i = 1 to m\n",
    "    * 从D中随机选择一个样本R\n",
    "    * 从R的同类样本集中找到R的k个最近邻$H_{j}(j = 1,2,...,k)$，从每个不同类样本集中找到k个最近邻$M_{j}(C)$;\n",
    "+ for A = 1 to N (all features)\n",
    "    * $W(A) = W(A)-\\sum_{j=1}^{k}diff(A,R,H_{j})/(mk)+\\sum_{C \\notin class(R)}[\\frac{p(C)}{1-p(class(R))} \\sum_{j=1}^{k}diff(A,R,M_{j}(C))]/(mk)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   N0  N1  N2  N3  N4  N5  N6  N7  N8  N9  ...    N11  N12  N13  N14  N15  \\\n",
      "0   0   0   2   1   1   0   0   2   0   1  ...      2    1    0    0    0   \n",
      "1   0   0   1   0   0   1   1   0   0   1  ...      0    1    0    0    0   \n",
      "2   0   0   0   1   0   2   0   0   0   0  ...      1    1    0    0    2   \n",
      "3   0   1   0   2   0   1   0   2   0   0  ...      1    1    0    1    1   \n",
      "4   0   0   1   1   0   0   0   1   1   0  ...      1    0    0    1    0   \n",
      "\n",
      "   N16  N17  P1  P2  class  \n",
      "0    0    0   0   1      1  \n",
      "1    0    0   0   0      1  \n",
      "2    2    0   0   0      1  \n",
      "3    0    1   1   0      1  \n",
      "4    0    1   1   0      1  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "0.793125134935\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skrebate import ReliefF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "genetic_data = pd.read_csv('GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.txt',\n",
    "                           sep='\\t')\n",
    "print genetic_data.head()\n",
    "features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n",
    "\n",
    "clf = make_pipeline(ReliefF(n_features_to_select=2, n_neighbors=100),\n",
    "                    RandomForestClassifier(n_estimators=100))\n",
    "\n",
    "print(np.mean(cross_val_score(clf, features, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('N0', '\\t', 12.5)\n",
      "('N1', '\\t', -332.0)\n",
      "('N2', '\\t', -496.0)\n",
      "('N3', '\\t', -498.0)\n",
      "('N4', '\\t', -238.0)\n",
      "('N5', '\\t', -559.0)\n",
      "('N6', '\\t', -605.5)\n",
      "('N7', '\\t', -408.5)\n",
      "('N8', '\\t', -369.0)\n",
      "('N9', '\\t', -569.5)\n",
      "('N10', '\\t', -29.0)\n",
      "('N11', '\\t', -359.0)\n",
      "('N12', '\\t', -394.0)\n",
      "('N13', '\\t', -195.0)\n",
      "('N14', '\\t', -184.0)\n",
      "('N15', '\\t', -403.5)\n",
      "('N16', '\\t', -516.5)\n",
      "('N17', '\\t', -462.5)\n",
      "('P1', '\\t', 6286.5)\n",
      "('P2', '\\t', 6612.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make sure to compute the feature importance scores from only your training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "\n",
    "fs = ReliefF()\n",
    "fs.fit(X_train, y_train)\n",
    "\n",
    "for feature_name, feature_score in zip(genetic_data.drop('class', axis=1).columns,\n",
    "                                       fs.feature_importances_):\n",
    "    print(feature_name, '\\t', feature_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "1. [特征选择算法-Relief](http://blog.csdn.net/ferrarild/article/details/18792613)\n",
    "2. [浅谈特征选择算法与Relief的实现](http://www.bubuko.com/infodetail-2156868.html)\n",
    "3. [特征选择之relief及reliefF算法](http://blog.csdn.net/littlely_ll/article/details/71614826)\n",
    "4. [ReliefF](https://github.com/EpistasisLab/scikit-rebate)\n",
    "5. [特征选择文档](http://scikit-learn.org/stable/modules/feature_selection.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
